# simulate data
n_obs <- 32
n_pred <- 4
data <- simulate_data(n_obs, n_pred, seed=1918)
outcome <- data$outcome
design <- data$design
# get MLE using analytical formula
linalg_out <- hiper_glm(design, outcome, model = "linear")
# get MLE using stats::optim
optim_out <- hiper_glm(design, outcome, model = "linear", option = list(mle_finder = "BFGS"))
# compare the two
expect_true(
are_all_close(coef(linalg_out), coef(optim_out))
)
})
library(testthat)
library(hiperglm)
test_that("linalg and optm least-sq coincide", {
# simulate data
n_obs <- 32
n_pred <- 4
data <- simulate_data(n_obs, n_pred, seed=1918)
outcome <- data$outcome
design <- data$design
# get MLE using analytical formula
linalg_out <- hiper_glm(design, outcome, model = "linear", option = list(mle_finder = "pseudoinverse"))
# get MLE using stats::optim
optim_out <- hiper_glm(design, outcome, model = "linear", option = list(mle_finder = "BFGS"))
# compare the two
expect_true(
are_all_close(coef(linalg_out), coef(optim_out))
)
})
library(testthat)
library(hiperglm)
test_that("linalg and optm least-sq coincide", {
# simulate data
n_obs <- 32
n_pred <- 4
data <- simulate_data(n_obs, n_pred, seed=1918)
outcome <- data$outcome
design <- data$design
# get MLE using analytical formula
linalg_out <- hiper_glm(design, outcome, model = "linear", option = list(mle_finder = "pseudoinverse"))
# get MLE using stats::optim
optim_out <- hiper_glm(design, outcome, model = "linear", option = list(mle_finder = "BFGS"))
# compare the two
expect_true(
are_all_close(coef(linalg_out), coef(optim_out))
)
})
test_that("linalg and optm least-sq coincide", {
# simulate data
n_obs <- 32
n_pred <- 4
data <- simulate_data(n_obs, n_pred, seed=1918)
outcome <- data$outcome
design <- data$design
# get MLE using analytical formula
linalg_out <- hiper_glm(design, outcome, model = "linear")
# get MLE using stats::optim
optim_out <- hiper_glm(design, outcome, model = "linear", option = list(mle_finder = "BFGS"))
# compare the two
expect_true(
are_all_close(coef(linalg_out), coef(optim_out))
)
})
test_that("linalg and optm least-sq coincide", {
# simulate data
n_obs <- 32
n_pred <- 4
data <- simulate_data(n_obs, n_pred, seed=1918)
outcome <- data$outcome
design <- data$design
# get MLE using analytical formula
linalg_out <- hiper_glm(design, outcome, model = "linear")
# get MLE using stats::optim
optim_out <- hiper_glm(design, outcome, model = "linear", option = list(mle_finder = "BFGS"))
# compare the two
expect_true(
are_all_close(coef(linalg_out), coef(optim_out))
)
})
test_that("linalg and optm least-sq coincide", {
# simulate data
n_obs <- 32
n_pred <- 4
data <- simulate_data(n_obs, n_pred, seed=1918)
outcome <- data$outcome
design <- data$design
# get MLE using analytical formula
linalg_out <- hiper_glm(design, outcome, model = "linear")
# get MLE using stats::optim
optim_out <- hiper_glm(design, outcome, model = "linear", option = list(mle_finder = "BFGS"))
# compare the two
expect_true(
are_all_close(coef(linalg_out), coef(optim_out))
)
})
rlang::last_trace()
test_that("linalg and optm least-sq coincide", {
# simulate data
n_obs <- 32
n_pred <- 4
data <- simulate_data(n_obs, n_pred, seed=1918)
outcome <- data$outcome
design <- data$design
# get MLE using analytical formula
linalg_out <- hiper_glm(design, outcome, model = "linear")
# get MLE using stats::optim
optim_out <- hiper_glm(design, outcome, model = "linear", option = list(mle_finder = "BFGS"))
# compare the two
expect_true(
are_all_close(coef(linalg_out), coef(optim_out))
)
})
test_that("linalg and optm least-sq coincide", {
# simulate data
n_obs <- 32
n_pred <- 4
data <- simulate_data(n_obs, n_pred, seed=1918)
outcome <- data$outcome
design <- data$design
# get MLE using analytical formula
linalg_out <- hiper_glm(design, outcome, model = "linear")
# get MLE using stats::optim
optim_out <- hiper_glm(design, outcome, model = "linear", option = list(mle_finder = "BFGS"))
# compare the two
expect_true(
are_all_close(coef(linalg_out), coef(optim_out))
)
})
?optim
test_that("linalg and optm least-sq coincide", {
# simulate data
n_obs <- 32
n_pred <- 4
data <- simulate_data(n_obs, n_pred, seed=1918)
outcome <- data$outcome
design <- data$design
# get MLE using analytical formula
linalg_out <- hiper_glm(design, outcome, model = "linear")
# get MLE using stats::optim
optim_out <- hiper_glm(design, outcome, model = "linear", option = list(mle_finder = "BFGS"))
# compare the two
expect_true(
are_all_close(coef(linalg_out), coef(optim_out))
)
})
n_obs <- 32
n_pred <- 4
data <- simulate_data(n_obs, n_pred, seed=1918)
outcome <- data$outcome
design <- data$design
solve(t(design) %*% design, t(design) %*% outcome)
optimal = stats::optim(par = rnorm(n = dim(design)[2]), fn = log_likelihood(design, outcome), gr = gradient(design, outcome), method = "BFGS")
optimal = stats::optim(par = rnorm(n = dim(design)[2]), fn = log_likelihood(par, design, outcome), gr = gradient(par, design, outcome), method = "BFGS")
optimal = stats::optim(par = rnorm(n = dim(design)[2]), fn = log_likelihood(design, outcome), gr = gradient(par, design, outcome), method = "BFGS")
optimal = stats::optim(par = rnorm(n = dim(design)[2]), fn = log_likelihood(design = design, outcome = outcome), gr = gradient(par, design, outcome), method = "BFGS")
optimal = stats::optim(par = rnorm(n = dim(design)[2]), fn = log_likelihood(design = design, outcome = outcome), gr = gradient(par, design, outcome), method = "BFGS")
rnorm(n = dim(design)[2])
par = rnorm(n = dim(design)[2])
log_likelihood(design = design, outcome = outcome)
optimal = stats::optim(par = rnorm(n = dim(design)[2]), fn = log_likelihood(par, design = design, outcome = outcome), gr = gradient(par, design, outcome), method = "BFGS")
log_likelihood(par, design = design, outcome = outcome)
gradient <- (1/noise_var)*t(residual) %*% design
log_likelihood <- function(par, design, outcome, noise_var = 1) {
yhat <- design %*% par
residual <- outcome - yhat
dimension <- dim(outcome)[1]
likelihood <- -(0.5*dimension*log(2*pi)) - (0.5*log(noise_var)) -((0.5/noise_var)*residual^2)
return(likelihood)
}
log_likelihood(par, design, outcome)
gradient(par, design, outcome)
gradient <- function(par, design, outcome, noise_var = 1) {
yhat <- design %*% par
residual <- outcome - yhat
gradient <- (1/noise_var)*t(residual) %*% design
return(gradient)
}
gradient(par, design, outcome)
test_that("linalg and optm least-sq coincide", {
# simulate data
n_obs <- 32
n_pred <- 4
data <- simulate_data(n_obs, n_pred, seed=1918)
outcome <- data$outcome
design <- data$design
# get MLE using analytical formula
linalg_out <- hiper_glm(design, outcome, model = "linear")
# get MLE using stats::optim
optim_out <- hiper_glm(design, outcome, model = "linear", option = list(mle_finder = "BFGS"))
# compare the two
expect_true(
are_all_close(coef(linalg_out), coef(optim_out))
)
})
n_obs <- 32
n_pred <- 4
data <- simulate_data(n_obs, n_pred, seed=1918)
outcome <- data$outcome
design <- data$design
hiper_glm(design, outcome, model = "linear")
test_that("linalg and optm least-sq coincide", {
# simulate data
n_obs <- 32
n_pred <- 4
data <- simulate_data(n_obs, n_pred, seed=1918)
outcome <- data$outcome
design <- data$design
# get MLE using analytical formula
linalg_out <- hiper_glm(design, outcome, model = "linear")
# get MLE using stats::optim
optim_out <- hiper_glm(design, outcome, model = "linear", mle_finder = "BFGS")
# compare the two
expect_true(
are_all_close(coef(linalg_out), coef(optim_out))
)
})
# get MLE using analytical formula
linalg_out <- hiper_glm(design, outcome, model = "linear")
linalg_out
coef(linalg_out)
coef.hglm(linalg_out)
coef(linalg_out)
coef = solve(t(design) %*% design, t(design) %*% outcome)
coef
test_that("linalg and optm least-sq coincide", {
# simulate data
n_obs <- 32
n_pred <- 4
data <- simulate_data(n_obs, n_pred, seed=1918)
outcome <- data$outcome
design <- data$design
# get MLE using analytical formula
linalg_out <- hiper_glm(design, outcome, model = "linear", mle_finder = "pseudoinverse")
# get MLE using stats::optim
optim_out <- hiper_glm(design, outcome, model = "linear", mle_finder = "BFGS")
# compare the two
expect_true(
are_all_close(coef(linalg_out), coef(optim_out))
)
})
# get MLE using analytical formula
linalg_out <- hiper_glm(design, outcome, model = "linear", mle_finder = "pseudoinverse")
coef(linalg_out)
# get MLE using analytical formula
linalg_out <- hiper_glm(design, outcome, model = "linear", mle_finder = "pseudoinverse")
coef(linalg_out)
# get MLE using stats::optim
optim_out <- hiper_glm(design, outcome, model = "linear", mle_finder = "BFGS")
optimal = stats::optim(par = rnorm(n = dim(design)[2]), fn = log_likelihood(par, design, outcome), gr = gradient(par, design, outcome), method = "BFGS")
log_likelihood <- function(par, design, outcome, noise_var = 1) {
yhat <- design %*% par
residual <- outcome - yhat
dimension <- dim(outcome)[1]
likelihood <- -(0.5*dimension*log(2*pi)) - (0.5*log(noise_var)) -((0.5/noise_var)*residual^2)
return(likelihood)
}
optimal = stats::optim(par = rnorm(n = dim(design)[2]), fn = log_likelihood(par, design, outcome), gr = gradient(par, design, outcome), method = "BFGS")
log_likelihood(par, design, outcome)
log_likelihood(c(100,100,100,100), design, outcome)
par
yhat <- design %*% par
yhat
residual <- outcome - yhat
residual
dimension <- dim(outcome)[1]
dimension
dim(outcome)
length(outcome)
log_likelihood <- function(par, design, outcome, noise_var = 1) {
yhat <- design %*% par
residual <- outcome - yhat
dimension <- length(outcome)
likelihood <- -(0.5*dimension*log(2*pi)) - (0.5*log(noise_var)) -((0.5/noise_var)*residual^2)
return(likelihood)
}
optimal = stats::optim(par = rnorm(n = dim(design)[2]), fn = log_likelihood(par, design, outcome), gr = gradient(par, design, outcome), method = "BFGS")
log_likelihood(par, design, outcome)
yhat <- design %*% par
residual <- outcome - yhat
dimension <- length(outcome)
likelihood <- -(0.5*dimension*log(2*pi)) - (0.5*log(noise_var)) -((0.5/noise_var)*residual^2)
noise_var = 1
likelihood <- -(0.5*dimension*log(2*pi)) - (0.5*log(noise_var)) -((0.5/noise_var)*residual^2)
likelihood
likelihood <- -(0.5*dimension*log(2*pi)) - (0.5*log(noise_var)) -((0.5/noise_var)*sum(residual^2))
likelihood
log_likelihood(par, design, outcome)
log_likelihood <- function(par, design, outcome, noise_var = 1) {
yhat <- design %*% par
residual <- outcome - yhat
dimension <- length(outcome)
likelihood <- -(0.5*dimension*log(2*pi)) - (0.5*log(noise_var)) -((0.5/noise_var)*sum(residual^2))
return(likelihood)
}
log_likelihood(par, design, outcome)
optimal = stats::optim(par = rnorm(n = dim(design)[2]), fn = log_likelihood(par, design, outcome), gr = gradient(par, design, outcome), method = "BFGS")
optimal = stats::optim(par = rnorm(n = dim(design)[2]), fn = log_likelihood(par, design, outcome), gr = gradient(par, design, outcome), method = "BFGS")
optimal = stats::optim(par = rnorm(n = dim(design)[2]), fn = log_likelihood(), design = design, outcome = outcome, gr = gradient(par, design, outcome), method = "BFGS")
optimal = stats::optim(par = rnorm(n = dim(design)[2]), fn = log_likelihood, design = design, outcome = outcome, gr = gradient(par, design, outcome), method = "BFGS")
optimal = stats::optim(par = rnorm(n = dim(design)[2]), fn = log_likelihood, design = design, outcome = outcome, gr = gradient, method = "BFGS")
optimal
coef = optimal$par
coef
test_that("linalg and optm least-sq coincide", {
# simulate data
n_obs <- 32
n_pred <- 4
data <- simulate_data(n_obs, n_pred, seed=1918)
outcome <- data$outcome
design <- data$design
# get MLE using analytical formula
linalg_out <- hiper_glm(design, outcome, model = "linear", mle_finder = "pseudoinverse")
# get MLE using stats::optim
optim_out <- hiper_glm(design, outcome, model = "linear", mle_finder = "BFGS")
# compare the two
expect_true(
are_all_close(coef(linalg_out), coef(optim_out))
)
})
# testing the gradient function
approx_grad <- function(func, x, dx = .Machine$double.eps^(1/3)) {
numerical_grad <- rep(0, length(x))
for (i in 1:length(x)) {
a = x[i]+dx
b = x[i]-dx
difference = func(a)-func(b)
numerical_grad[i] = difference/(2*dx)
}
return(numerical_grad)
}
# testing the gradient function
approx_grad <- function(func, x, dx = .Machine$double.eps^(1/3)) {
numerical_grad <- rep(0, length(x))
for (i in 1:length(x)) {
a = x[i]+dx
b = x[i]-dx
difference = func(a)-func(b)
numerical_grad[i] = difference/(2*dx)
}
return(numerical_grad)
}
set.seed(410)
n_param <- 4
X <- matrix(rnorm(2 * n_param^2), nrow = 2 * n_param, ncol = n_param)
Sigma_inv <- t(X) %*% X
gaussian_logp <- function(x, Sigma_inv) {
logp <- - .5 * t(x) %*% Sigma_inv %*% x
return(logp)
}
x <- c(3, 1, 4, 1)
approx_grad <- function(func, par, dx = .Machine$double.eps^(1/3)) {
gradient_vector <- rep(0, length(par))
for (i in 1:length(par)) {
a = par[i]+dx
b = par[i]-dx
difference = func(a)-func(b)
numerical_grad[i] = difference/(2*dx)
}
return(numerical_grad)
}
# simulate data
n_obs <- 32
n_pred <- 4
data <- simulate_data(n_obs, n_pred, seed=1918)
outcome <- data$outcome
design <- data$design
analytical_grad <- gradient(par, design, outcome)
numerical_grad <- approx_grad(function(par) log_likelihood(par, design, outcome), par)
approx_grad <- function(func, par, dx = .Machine$double.eps^(1/3)) {
gradient_vector <- rep(0, length(par))
for (i in 1:length(par)) {
a = par[i]+dx
b = par[i]-dx
difference = func(a)-func(b)
numerical_grad[i] = difference/(2*dx)
}
return(numerical_grad)
}
# simulate data
n_obs <- 32
n_pred <- 4
data <- simulate_data(n_obs, n_pred, seed=1918)
outcome <- data$outcome
design <- data$design
par <- c(1, 2, 3, 4)
analytical_grad <- gradient(par, design, outcome)
numerical_grad <- approx_grad(function(par) log_likelihood(par, design, outcome), par)
approx_grad <- function(func, par, dx = .Machine$double.eps^(1/3)) {
gradient_vector <- rep(0, length(par))
for (i in 1:length(par)) {
a = par[i]+dx
b = par[i]-dx
difference = func(a)-func(b)
numerical_grad[i] = difference/(2*dx)
}
return(numerical_grad)
}
n_obs <- 32
n_pred <- 4
data <- simulate_data(n_obs, n_pred, seed=1918)
outcome <- data$outcome
design <- data$design
par <- c(1, 2, 3, 4)
analytical_grad <- gradient(par, design, outcome)
numerical_grad <- approx_grad(function(par) log_likelihood(par, design, outcome), par)
design
n_obs <- 32
n_pred <- 4
data <- simulate_data(n_obs, n_pred, seed=1918)
outcome <- data$outcome
design <- data$design
par = rnorm(n = dim(design)[2])
analytical_grad <- gradient(par, design, outcome)
numerical_grad <- approx_grad(function(par) log_likelihood(par, design, outcome), par)
analytical_grad <- gradient(par, design, outcome)
analytical_grad
approx_grad <- function(func, par, dx = .Machine$double.eps^(1/3)) {
gradient_vector <- rep(0, length(par))
for (i in 1:length(par)) {
a_vector <- par
b_vector <- par
a_vector[i] = par[i]+dx
b_vector[i] = par[i]-dx
gradient_vector[i] = (func(a, design, outcome)-func(b, design, outcome))/(2*dx)
}
return(gradient_vector)
}
# simulate data
n_obs <- 32
n_pred <- 4
data <- simulate_data(n_obs, n_pred, seed=1918)
outcome <- data$outcome
design <- data$design
par = rnorm(n = dim(design)[2])
analytical_grad <- gradient(par, design, outcome)
numerical_grad <- approx_grad(function(par) log_likelihood(par, design, outcome), par)
approx_grad <- function(func, par, dx = .Machine$double.eps^(1/3)) {
gradient_vector <- rep(0, length(par))
for (i in 1:length(par)) {
a_vector <- par
b_vector <- par
a_vector[i] = par[i]+dx
b_vector[i] = par[i]-dx
gradient_vector[i] = (func(a)-func(b))/(2*dx)
}
return(gradient_vector)
}
# simulate data
n_obs <- 32
n_pred <- 4
data <- simulate_data(n_obs, n_pred, seed=1918)
outcome <- data$outcome
design <- data$design
par = rnorm(n = dim(design)[2])
analytical_grad <- gradient(par, design, outcome)
numerical_grad <- approx_grad(function(par) log_likelihood(par, design, outcome), par)
approx_grad <- function(func, par, dx = .Machine$double.eps^(1/3)) {
gradient_vector <- rep(0, length(par))
for (i in 1:length(par)) {
a_vector <- par
b_vector <- par
a_vector[i] = par[i]+dx
b_vector[i] = par[i]-dx
gradient_vector[i] = (func(a_vector)-func(b_vector))/(2*dx)
}
return(gradient_vector)
}
# simulate data
n_obs <- 32
n_pred <- 4
data <- simulate_data(n_obs, n_pred, seed=1918)
outcome <- data$outcome
design <- data$design
par = rnorm(n = dim(design)[2])
analytical_grad <- gradient(par, design, outcome)
numerical_grad <- approx_grad(function(par) log_likelihood(par, design, outcome), par)
testthat::expect_true(are_all_close(
analytical_grad, numerical_grad, abs_tol = Inf, rel_tol = 1e-3
))
testthat::expect_true(are_all_close(
analytical_grad, numerical_grad, abs_tol = Inf, rel_tol = 1e-3
))
testthat::expect_true(are_all_close(
analytical_grad, numerical_grad, abs_tol = Inf, rel_tol = 1e-3
))
test_that("linalg and optm least-sq coincide", {
# simulate data
n_obs <- 32
n_pred <- 4
data <- simulate_data(n_obs, n_pred, seed=1918)
outcome <- data$outcome
design <- data$design
# get MLE using analytical formula
linalg_out <- hiper_glm(design, outcome, model = "linear", mle_finder = "pseudoinverse")
# get MLE using stats::optim
optim_out <- hiper_glm(design, outcome, model = "linear", mle_finder = "BFGS")
# compare the two
expect_true(
are_all_close(coef(linalg_out), coef(optim_out))
)
})
analytical_grad <- gradient(par, design, outcome)
analytical_grad
numerical_grad <- approx_grad(function(par) log_likelihood(par, design, outcome), par)
numerical_grad
testthat::expect_true(are_all_close(
analytical_grad, numerical_grad, abs_tol = Inf, rel_tol = 1e-3
))
